# Phase III: Prototypes and User Testing

## Introduction

In Phase III, our team focused on conducting user studies based on our wireframe prototypes, which were modified based on feedback from the previous two phases. This phase aimed to assess how the wireframe performed with individuals unfamiliar with our user study, and to gather data for further improving the wireframe.

## Methods

For this study, we used a user study, looking at how people used our wireframe prototype (which you can find linked on the home page). The user study had both questions at the beginning and tasks for people to do, and we had them think aloud, to see how they experienced the app. We also had the partcipants use the think-aloud protocol, where participants verbalized their thoughts and actions while navigating the site. This method allowed us to gain insights into their decision-making processes, navigation, and perceptions of the interface, helped us write detailed notes.

**Preliminary Questions:**

We started by asking people about their past experience with health apps and what they expected from an app that checks heart health risk:
  
  > Have you ever used a health or medical application or website before?
  
  > What was your experience like, and what features did you find helpful or frustrating?
  
  > What do you expect when using an app designed to check your heart health risk?

Then, we gave people four tasks to do using the wireframe prototype. We wrote down how they moved through the app and what they said as they were thinking aloud.

**Task 1: Check Risk Without an Account**

Participants were instructed to imagine a friend recommending the site for checking heart risk percentage without needing an account. We observed how they located and used the risk assessment form.

After they finished, we asked:

  > Able to navigate the site, enter biometrics, and get your risks: ⃞ Yes ⃞ No
  
  > On a scale from 1 to 5, where 1 is "Very Difficult" and 5 is "Very Easy", how would you rate completing this task on this prototype? Why?

These questions aimed to gather quantitative data on task completion and perceived difficulty, along with qualitative data explaining their ease-of-use rating.

**Task 2: Check Risk With an Account**

Participants were required to create an account and then enter their biometrics to obtain their risk assessment. This task assessed the account creation process and subsequent navigation.

After they finished, we asked:

  > Able to create an account, enter biometrics, and get your risks: ⃞ Yes ⃞ No
  
  > On a scale from 1 to 5, where 1 is "Very Difficult" and 5 is "Very Easy", how would you rate completing this task on this prototype? Why?

These questions gave us numbers on whether they could do the task and how hard it was, and words on why they rated it that way.

**Task 3: Understanding the AI Model and Results**

People were told to go to the 'About' page to learn about the model, then enter their information, submit it, and look at their results. This task checked if people understood the model, trusted the information, and found the results easy to understand.

After they finished, we asked:

  > Able to understand the AI model from the ‘About Page’, enter biometrics, and understand the dashboard: ⃞ Yes ⃞ No
  
  > On a scale from 1 to 5, where 1 is "Very Difficult" and 5 is "Very Easy", how would you rate completing this task on this prototype? Why?
  
  > Did the information on the page make you trust the AI model more?
  
  > Was the results page clear and easy to understand? What did you find confusing?

These questions assessed task completion, perceived difficulty, trust in the AI model, and clarity/usability of the dashboard.

**Task 4: Looking at Extra Resources**

Participants were instructed to navigate the site and explore available help pages and resources. This task aimed to evaluate the discoverability and usefulness of these resources.

After they finished, we asked:

  > Able to successfully identify and explore additional resources or features beyond basic risk assessment: ⃞ Yes ⃞ No
  
  > On a scale from 1 to 5, where 1 is "Very Difficult" and 5 is "Very Easy", how would you rate completing this task on this prototype? Why?
  
  > Did you find these resources helpful or relevant to your health concerns?

These questions assessed task completion, perceived difficulty, and the helpfulness/relevance of the resources.

## Findings

The user study of the heart disease risk predictor prototype revealed generally positive user feedback regarding navigability and usability. Participants found the prototype easy to use, with task difficulty consistently rated between 4 and 5. The risk assessment tasks, both with and without account creation, and exploring resources were particularly well-received.

The majority of participants (Participants 1, 2, 3, and 4) found the prototype easy to use, see here:

![Participants Rating of Tasks](https://github.com/user-attachments/assets/9bbf2baa-8462-43b7-b9dc-6e65c569d720)

Task 3, "Understanding the AI Model," was the most challenging for participants. Participant 2, in particular, expressed significant difficulty with the "About" page, stating that an "ordinary person would not be able to understand" it and rated the task with a 2 for difficulty.

The "About" page, which explains the AI model, was a key area of concern. Participant 6 indicated that the information about the model was unclear, including how it was developed and how it works, and specifically mentioned confusion about the confusion matrix.

The dashboard was generally well-received. Participant 2 described it as "neat and organized," and Participants 3 and 4 also found it clear. However, Participant 5 noted that the dashboard had a lot of empty space before viewing a past diagnosis.

Participants 1, 3, 4, and 5 all found the overall process of using the application to be easy and straightforward. Participant 5 also liked the "navigate back" button on the "how to measure" page.

In summary, while the application was generally found to be easily accessible, the "About" page and, to a lesser extent, the dashboard design, represent areas needing improvement, with specific participants highlighting the difficulties and potential solutions.

[Link to Spreadsheet](https://docs.google.com/spreadsheets/d/1KdUS-SbOOybxD6Isx_X-41SRt3kASANTLCVkW2xjnd0/edit?gid=0#gid=0)

## Conclusions

!!! Discoveries derived from the methods and their findings. Interpret how the findings translate into new insights into UX design recommendations. Describe those recommendations and how they should shape future work. In this section, include the new design recommendations based on the latest user insights. !!!

- [In conclusions] The interpretation of your findings, including both (A) recommended changes to improve the user experience and (B) aspects of the design that are affirmed to remain as-is

## Caveats

Our study has several limitations that should be considered when interpreting the findings. First, our participant pool primarily consisted of Computer Science students, and many participants were known to the researchers. This introduces potential biases, as their familiarity with technology and the research team could influence their interactions with the prototype and their feedback. Second, the sample was predominantly male, which limits the generalizability of our results to a broader population. Finally, our study may not have uncovered all potential usability issues or user preferences due to the specific tasks and questions we employed.
